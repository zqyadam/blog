<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Going deeper with convolutions摘要 Abstract We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://blog.zozo.site/2018/07/12/test/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Going deeper with convolutions摘要 Abstract We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://blog.zozo.site/1529821757122.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions/1529821776472.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions.assets/1529822771040.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions.assets/1529823104031.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions.assets/6.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions.assets/1529823970295.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions.assets/1529823993150.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions/1529824251348.png">
<meta property="og:image" content="http://blog.zozo.site/2018/07/12/test/Going%20deeper%20with%20convolutions/1529824266331.png">
<meta property="og:updated_time" content="2018-07-12T12:17:31.107Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="Going deeper with convolutions摘要 Abstract We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification">
<meta name="twitter:image" content="http://blog.zozo.site/1529821757122.png">






  <link rel="canonical" href="http://blog.zozo.site/2018/07/12/test/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title> | Hexo</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.zozo.site/2018/07/12/test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-12 20:01:35 / 修改时间：20:17:31" itemprop="dateCreated datePublished" datetime="2018-07-12T20:01:35+08:00">2018-07-12</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h1><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><blockquote>
<p>We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.</p>
</blockquote>
<p>我们提出了一个代号为Inception的深度卷积神经网络架构，负责设置2014年ImageNet大规模视觉识别挑战（ILSVRC14）中分类和检测的最新技术状态。 这种架构的主要特点是提高了网络内部计算资源的利用率。 这是通过精心设计的设计实现的，可以在保持计算预算不变的同时增加网络的深度和宽度。 为了优化质量，架构决策基于Hebbian原理和多尺度处理的直觉。 我们提交给ILSVRC14的一个具体化身被称为GoogLeNet，这是一个22层深的网络，其质量在分类和检测方面进行评估。</p>
<h2 id="1-介绍-Introduction"><a href="#1-介绍-Introduction" class="headerlink" title="1 介绍 Introduction"></a>1 介绍 Introduction</h2><blockquote>
<p>In the last three years, mainly due to the advances of deep learning, more concretely convolutional networks , the quality of image recognition and object detection has been progressing at a dramatic pace. One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al from two years ago, while being significantly more accurate. The biggest gains in object-detection have not come from the utilization of deep networks alone or bigger models, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick.</p>
</blockquote>
<p>在过去的三年里，主要是由于深度学习的进展，更具体的卷积网络，图像识别和目标检测的质量一直在飞速发展。一个令人鼓舞的消息是，这些进展大部分不仅是更强大的硬件，更大的数据集和更大的模型的结果，而且主要是新思想，算法和改进网络架构的结果。例如，除了用于检测目的的同一竞赛的分类数据集之外，没有使用新的数据源，例如ILSVRC 2014比赛中的最佳条目。我们提交给ILSVRC 2014的GoogLeNet实际上使用的参数比两年前Krizhevsky等人获胜的体系结构少了12倍，同时显着更准确。目标检测的最大收益不是来自单独使用深度网络或更大型号，而是来自深层架构和经典计算机视觉的协同作用，如Girshick的R-CNN算法。</p>
<blockquote>
<p>Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms – especially their power and memory use – gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.</p>
</blockquote>
<p>另一个值得注意的因素是，随着移动和嵌入式计算的不断推动，我们算法的效率 - 尤其是其功耗和内存使用 - 变得越来越重要。 值得注意的是，本文提出的深层架构设计的考虑因素包含了这个因素，而不是对准确度数值的纯粹固定。 对于大多数实验来说，这些模型的设计目的是在推理时保持15亿次乘加的计算预算，以便它们不会成为纯粹的学术好奇心，但可以投入到现实世界的使用中，甚至 在大数据集上，以合理的成本。</p>
<blockquote>
<p>In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al  in conjunction with the famous “we need to go deeper” internet meme . In our case, the word “deep” is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the “Inception module” and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of  while taking inspiration and guidance from the theoretical work by Arora et al . The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, on which it significantly outperforms the current state of the art.</p>
</blockquote>
<p>在本文中，我们将重点介绍一种高效的计算机视觉深层神经网络体系结构，代号为Inception，由Lin等人在着名的“我们需要深入”的互联网模型的网络论文中从网络论文中得到它的名字。 在我们的例子中，“深”这个词有两个不同的含义：首先，就我们引入组织的一个新层次而言，它是以“先入先出”的形式出现的，也是更直接意义上的增加的网络 深度。 一般来说，人们可以将启蒙模型视为逻辑高潮，同时从Arora等人的理论工作中获得灵感和指导。 该体系结构的优势在ILSVRC 2014分类和检测挑战中进行了实验验证，在这种挑战下它显着优于当前的技术水平。</p>
<h2 id="2-相关工作-Related-Work"><a href="#2-相关工作-Related-Work" class="headerlink" title="2 相关工作 Related Work"></a>2 相关工作 Related Work</h2><blockquote>
<p>Starting with LeNet-5 , convolutional neural networks (CNN) have typically had a standard structure – stacked convolutional layers (optionally followed by contrast normalization and max- pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge . For larger datasets such as Imagenet, the recent trend has been to increase the number of layers and layer size , while using dropout  to address the problem of overfitting.</p>
</blockquote>
<p>从LeNet-5开始，卷积神经网络（CNN）通常具有标准结构 - 堆叠卷积层（可选地随后是对比度归一化和最大量化）之后是一个或多个完全连接的层。 这种基本设计的变体在图像分类文献中很普遍，并且在MNIST，CIFAR和最显着的ImageNet分类挑战上取得了最好的结果。 对于较大的数据集，如Imagenet，最近的趋势是增加层数和图层大小，同时使用丢失来解决过度拟合问题。</p>
<blockquote>
<p>Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as  has also been successfully employed for localization , object detection  and human pose estimation . Inspired by a neuroscience model of the primate visual cortex, Serre et al. use a series of fixed Gabor filters of different sizes in order to handle multiple scales, similarly to the Inception model. However, contrary to the fixed 2-layer deep model of , all filters in the Inception model are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.</p>
</blockquote>
<p>尽管担心最大汇聚层会导致精确空间信息的丢失，但同样的卷积网络体系结构也已成功用于定位，目标检测和人体姿态估计。 受灵长类动物视觉皮层的神经科学模型的启发，Serre等人 使用一系列不同尺寸的固定Gabor滤波器来处理多个尺度，与Inception模型类似。 然而，与固定的2层深层模型相反，Inception模型中的所有滤波器都被学习到了。 此外，初始层重复多次，在GoogLeNet模型的情况下导致一个22层深的模型。</p>
<blockquote>
<p>Network-in-Network is an approach proposed by Lin in order to increase the representational power of neural networks. When applied to convolutional layers, the method could be viewed as additional 1×1 convolutional layers followed typically by the rectified linear activation . This enables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our architecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without significant performance penalty.</p>
</blockquote>
<p>网络网络是Lin提出的一种方法，用于提高神经网络的表示能力。 当应用于卷积层时，该方法可以被看作额外的1×1卷积层，其后通常是经整流的线性激活。 这使它可以很容易地集成到当前的CNN管道中。 我们在架构中大量使用这种方法。 然而，在我们的设置中，1×1卷积具有双重目的：最关键的是，它们主要用作降维模块以消除计算瓶颈，否则会限制我们网络的规模。 这不仅允许增加深度，还允许我们网络的宽度没有显着的性能损失。</p>
<blockquote>
<p>The current leading approach for object detection is the Regions with Convolutional Neural Net- works (R-CNN) proposed by Girshick . R-CNN decomposes the overall detection problem into two subproblems: to first utilize low-level cues such as color and superpixel consistency for potential object proposals in a category-agnostic fashion, and to then use CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.</p>
</blockquote>
<p>目前主要的物体检测方法是由Girshick提出的带有卷积神经网络的区域（R-CNN）。 R-CNN将整体检测问题分解为两个子问题：首先利用颜色和超像素一致性等低级提示以类别不可知的方式提供潜在的对象提议，然后使用CNN分类器在这些位置识别对象类别。 这种两阶段方法利用边界框分割与低级线索的精确性，以及最先进的CNN的高度强大的分类能力。 我们在检测提交中采用了类似的流水线，但在两个阶段都有所改进，例如更高对象边界框回忆的多框预测和更好地对边界框提议进行分类的集合方法。</p>
<h2 id="3-动机和高层次考虑-Motivation-and-High-Level-Considerations"><a href="#3-动机和高层次考虑-Motivation-and-High-Level-Considerations" class="headerlink" title="3 动机和高层次考虑 Motivation and High Level Considerations"></a>3 动机和高层次考虑 Motivation and High Level Considerations</h2><blockquote>
<p>The most straightforward way of improving the performance of deep neural networks is by increasing their size. This includes both increasing the depth – the number of levels – of the network and its width: the number of units at each level. This is as an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However this simple solution comes with two major drawbacks.</p>
</blockquote>
<p>提高深度神经网络性能的最直接的方法是增加它们的大小。 这包括增加网络的深度 - 层数 - 宽度：每层的单元数量。 这是训练更高质量模型的一种简单而安全的方式，尤其是考虑到大量标记的训练数据的可用性。 但是，这个简单的解决方案有两个主要缺点。</p>
<blockquote>
<p>Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. This can become a major bottleneck, since the creation of high quality training sets can be tricky and expensive, especially if expert human raters are necessary to distinguish between fine-grained visual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated by Figure 1.</p>
</blockquote>
<p>更大的尺寸通常意味着更多的参数，这使得放大的网络更容易出现过度拟合，特别是如果训练集中标记示例的数量有限。 这可能成为一个主要的瓶颈，因为创建高质量的训练集可能会非常棘手和昂贵，特别是如果需要专业人员评估者来区分像ImageNet中那样的细粒度视觉类别（即使在1000级ILSVRC子集中） 如图1所示。</p>
<p><img src="/1529821757122.png" alt="(Going%20deeper%20with%20convolutions/1529821757122.png) Siberian husky"></p>
<p>(a) Siberian husky</p>
<p><img src="Going%20deeper%20with%20convolutions/1529821776472.png" alt="1529821776472"></p>
<p>(b) Eskimo dog</p>
<p>图1：ILSVRC 2014分类挑战的1000个类中的两个不同类。</p>
<blockquote>
<p>Another drawback of uniformly increased network size is the dramatically increased use of computational resources. For example, in a deep vision network, if two convolutional layers are chained, any uniform increase in the number of their filters results in a quadratic increase of computation. If the added capacity is used inefficiently (for example, if most weights end up to be close to zero), then a lot of computation is wasted. Since in practice the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size, even when the main objective is to increase the quality of results.</p>
</blockquote>
<p>统一增加网络大小的另一个缺点是计算资源的使用大大增加。 例如，在深度视觉网络中，如果两个卷积层链接在一起，其滤波器数量的任何均匀增加导致计算的二次增加。 如果增加的容量被低效地使用（例如，如果大多数权重最终接近于零），那么大量的计算被浪费了。 由于在实践中计算预算总是有限的，即使主要目标是提高结果质量，计算资源的有效分配也优于不加区分地增加大小。</p>
<blockquote>
<p>The fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions. Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora . Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well known Hebbian principle – neurons that fire together, wire together – suggests that the underlying idea is applicable even under less strict conditions, in practice.</p>
</blockquote>
<p>解决这两个问题的根本途径是最终从完全连接转向稀疏连接的体系结构，即使在卷积内也是如此。 除了模仿生物系统之外，由于Arora的开创性工作，这也将具有更坚实的理论基础。 他们的主要结果表明，如果数据集的概率分布可以用一个大的，非常稀疏的深度神经网络表示，那么可以通过分析最后一层的激活的相关统计来逐层构建最佳网络拓扑， 聚类与高度相关输出的神经元。 虽然严格的数学证明需要非常强的条件，但这一陈述与众所周知的赫布斯原理（共同发射，连接在一起的神经元）共鸣的事实表明，即使在不太严格的条件下，实践中也可以使用基本思想。</p>
<blockquote>
<p>On the downside, todays computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by 100×, the overhead of lookups and cache misses is so dominant that switching to sparse matrices would not pay off. The gap is widened even further by the use of steadily improving, highly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware . Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since in order to break the symmetry and improve learning, the trend changed back to full connections with in order to better optimize parallel computing. The uniformity of the structure and a large number of filters and greater batch size allow for utilizing efficient dense computation.</p>
</blockquote>
<p>不利的一面是，在非均匀稀疏数据结构的数值计算方面，今天的计算基础设施效率很低。即使算术运算的次数减少了100倍，查找和缓存未命中的开销也是如此占优势，以至于切换到稀疏矩阵将无济于事。通过使用稳定改进的，高度调整的数字库，可以实现极快的密集矩阵乘法，利用底层CPU或GPU硬件的微小细节，进一步扩大了差距。而且，非均匀稀疏模型需要更复杂的工程和计算基础设施。大多数当前面向视觉的机器学习系统仅利用卷积来利用空间域中的稀疏性。但是，卷积是作为较早层中的补丁的密集连接的集合来实现的。 ConvNets传统上在特征维度中使用随机和稀疏连接表，因为为了打破对称性并改善学习，趋势变回到完全连接，以便更好地优化并行计算。结构的均匀性和大量过滤器以及更大的批量大小允许利用高效密集计算。</p>
<blockquote>
<p>This raises the question whether there is any hope for a next, intermediate step: an architecture that makes use of the extra sparsity, even at filter level, as suggested by the theory, but exploits our current hardware by utilizing computations on dense matrices. The vast literature on sparse matrix computations suggests that clustering sparse matrices into relatively dense submatrices tends to give state of the art practical performance for sparse matrix multiplication. It does not seem far-fetched to think that similar methods would be utilized for the automated construction of non-uniform deep-learning architectures in the near future.</p>
</blockquote>
<p>这就产生了一个问题，即是否有希望进入下一个中间步骤：即使在滤波器级别使用额外稀疏性的架构，正如理论所建议的那样，但是通过利用密集矩阵上的计算来利用我们当前的硬件。 有关稀疏矩阵计算的大量文献表明，将稀疏矩阵聚类到相对密集的子矩阵中往往会为稀疏矩阵乘法提供最先进的实际性能。 认为在不久的将来自动化构建非统一的深度学习架构将使用类似的方法似乎并不遥远。</p>
<blockquote>
<p>The Inception architecture started out as a case study of the first author for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by [2] for vision networks and covering the hypothesized outcome by dense, readily available components. Despite being a highly speculative undertaking, only after two iterations on the exact choice of topology, we could already see modest gains against the reference architecture based on [12]. After further tuning of learning rate, hyperparameters and improved training methodology, we established that the resulting Inception architecture was especially useful in the context of localization and object detection as the base network for [6] and [5]. Interestingly, while most of the original architectural choices have been questioned and tested thoroughly, they turned out to be at least locally optimal.</p>
</blockquote>
<p>Inception架构开始作为第一作者的案例研究，用于评估复杂的网络拓扑构建算法的假设输出，该算法试图逼近[2]为视觉网络所隐含的稀疏结构，并通过密集的，容易获得的假设覆盖假设的结果 组件。 尽管是一个高度推测性的工作，但只有在精确选择拓扑的两次迭代之后，我们才可以看到基于[12]的参考架构的适度增益。 在进一步调整学习速率，超参数和改进的训练方法之后，我们确定，由此产生的Inception架构在本地化和目标检测作为[6]和[5]的基础网络方面特别有用。 有趣的是，尽管大部分原始架构选择都经过了彻底的质疑和测试，但他们证明至少在本地是最佳的。</p>
<blockquote>
<p>One must be cautious though: although the proposed architecture has become a success for computer vision, it is still questionable whether its quality can be attributed to the guiding principles that have lead to its construction. Making sure would require much more thorough analysis and verification: for example, if automated tools based on the principles described below would find similar, but better topology for the vision networks. The most convincing proof would be if an automated system would create network topologies resulting in similar gains in other domains using the same algorithm but with very differently looking global architecture. At very least, the initial success of the Inception architecture yields firm motivation for exciting future work in this direction.</p>
</blockquote>
<p>但必须谨慎：尽管提出的架构已经成为计算机视觉的成功，但它的质量是否可以归因于导致其构建的指导原则仍然值得怀疑。 确保需要更彻底的分析和验证：例如，如果基于下述原理的自动化工具会为视觉网络找到相似但更好的拓扑结构。 最有说服力的证据是，如果一个自动化系统能够创建网络拓扑结构，从而在使用相同算法的其他领域获得类似的收益，但在全球架构上看起来却不一样。 至少，初始架构的最初成功为激励未来在这个方向上的工作提供了坚定的动力。</p>
<h2 id="4-架构细节-Architectural-Details"><a href="#4-架构细节-Architectural-Details" class="headerlink" title="4 架构细节 Architectural Details"></a>4 架构细节 Architectural Details</h2><blockquote>
<p>The main idea of the Inception architecture is based on finding out how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components. Note that assuming translation invariance means that our network will be built from convolutional building blocks. All we need is to find the optimal local construction and to repeat it spatially. Arora suggests a layer-by layer construction in which one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation. These clusters form the units of the next layer and are connected to the units in the previous layer. We assume that each unit from the earlier layer corresponds to some region of the input image and these units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units would concentrate in local regions. This means, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as suggested in [12]. However, one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches, and there will be a decreasing number of patches over larger and larger regions. In order to avoid patchalignment issues, current incarnations of the Inception architecture are restricted to filter sizes 1×1, 3×3 and 5×5, however this decision was based more on convenience rather than necessity. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage. Additionally, since pooling operations have been essential for the success in current state of the art convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (see Figure 2(a)).</p>
</blockquote>
<p>初始体系结构的主要思想是基于找出卷积视觉网络中的最佳局部稀疏结构如何可以通过容易获得的密集分量来近似和覆盖。请注意，假设转换不变意味着我们的网络将从卷积构建块构建。我们需要的是找到最佳的局部构造并在空间上重复它。 Arora提出了一种逐层构造，其中应该分析最后一层的相关统计并将它们聚类为具有高度相关性的单元组。这些簇形成下一层的单元并连接到前一层的单元。我们假设来自较早层的每个单元对应于输入图像的一些区域，并且这些单元被分组成滤波器组。在较低层（接近输入的那些层）相关单元将集中在本地区域。这意味着，我们最终将有大量的聚类集中在一个区域中，并且它们可以在下一层被1×1卷积层覆盖，如[12]中所述。然而，人们也可以预期，在更大的斑块上卷积可以覆盖的空间分布更多的簇的数量将会减少，并且越来越大的区域将会有越来越多的斑块。为了避免匹配问题，初始体系结构的当前版本被限制为1×1,3×3和5×5的滤波器大小，但是，这个决定更多地取决于便利性而非必要性。这也意味着所建议的架构是所有这些层的组合，其输出滤波器组连接成单个输出矢量，形成下一阶段的输入。此外，由于汇集操作对于当前现有技术水平的卷积网络的成功至关重要，因此它建议在每个这样的阶段添加替代的并行汇集路径也应该具有额外的有益效果（参见图2（a））。</p>
<blockquote>
<p>As these “Inception modules” are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease suggesting that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.</p>
</blockquote>
<p>由于这些“起始模块”堆叠在一起，因此它们的输出相关统计量必然会发生变化：由于较高层的特征会被较高层捕获，因此它们的空间浓度预计会减少，这表明3×3和 随着我们移动到更高层次，5×5卷积应该会增加。</p>
<p><img src="Going%20deeper%20with%20convolutions.assets/1529822771040.png" alt="1529822771040"></p>
<blockquote>
<p>One big problem with the above modules, at least in this na¨ ıve form, is that even a modest number of 5×5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters. This problem becomes even more pronounced once pooling units are added to the mix: their number of output filters equals to the number of filters in the previous stage. The merging of the output of the pooling layer with the outputs of convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage. Even while this architecture might cover the optimal sparse structure, it would do it very inefficiently, leading to a computational blow up within a few stages.</p>
</blockquote>
<p>至少在这种初始形式中，上述模块的一个大问题是，即使是数量最少的5×5卷积，在具有大量过滤器的卷积层的顶部也会过于昂贵。 一旦将混合单元添加到混合中，这个问题变得更加明显：它们的输出过滤器数量等于前一阶段中的过滤器数量。 合并层的输出与卷积层输出的合并将导致阶段到阶段的输出数量不可避免地增加。 即使这种架构可能覆盖最佳的稀疏结构，它也会非常低效地执行，导致在几个阶段内发生计算性爆炸。</p>
<blockquote>
<p>This leads to the second idea of the proposed architecture: judiciously applying dimension reduc- tions and projections wherever the computational requirements would increase too much otherwise. This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch. However, embeddings represent information in a dense, compressed form and compressed information is harder to model. We would like to keep our representation sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation which makes them dual-purpose. The final result is depicted in Figure 2(b).</p>
</blockquote>
<p>这导致了所提出体系结构的第二个想法：在计算需求增加太多的情况下，明智地应用降维和预测。 这是基于嵌入成功的：甚至低维嵌入可能包含大量关于较大图像补丁的信息。 但是，嵌入以密集的压缩形式表示信息，并且压缩的信息很难建模。 我们希望在大多数地方保持我们的表示稀疏（如[2]的条件所要求的那样），并且只有在必须集中汇总信号时才压缩信号。 也就是说，在昂贵的3×3和5×5卷积之前，使用1×1卷积来计算减少量。 除了作为减少使用，他们还包括使用整流线性激活，这使得他们两用。 最终结果如图2（b）所示。</p>
<blockquote>
<p>In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For technical reasons (memory efficiency during training), it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion. This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current implementation.</p>
</blockquote>
<p>一般来说，Inception网络是由上述类型的模块组成的一个网络，彼此堆叠在一起，偶尔使用步长为2的最大池层将网格的分辨率减半。 出于技术原因（培训期间的存储效率），开始仅在较高层使用Inception模块，而以传统卷积方式保留较低层时似乎是有益的。 这不是绝对必要的，只是反映了我们目前实施中的一些基础设施效率低下问题。</p>
<blockquote>
<p>One of the main beneficial aspects of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow-up in computational complexity. The ubiquitous use of dimension reduction allows for shielding the large number of input filters of the last stage to the next layer, first reducing their dimension before convolving over them with a large patch size. Another practically useful aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously.</p>
</blockquote>
<p>这种架构的主要优势之一是它可以显着增加每个阶段的单元数量，而不会在计算复杂性方面造成无法控制的爆炸。 尺寸缩减的普遍使用允许将最后一个阶段的大量输入滤波器屏蔽到下一个层，首先减小它们的尺寸，然后在大尺寸的片上进行卷积。 这种设计的另一个实际有用的方面是它符合直觉，即视觉信息应该以不同的尺度进行处理，然后进行聚合，以便下一阶段可以同时从不同尺度提取特征。</p>
<blockquote>
<p>The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties. Another way to utilize the inception architecture is to create slightly inferior, but computationally cheaper versions of it. We have found that all the included the knobs and levers allow for a controlled balancing of computational resources that can result in networks that are 2−3× faster than similarly performing networks with non-Inception architecture, however this requires careful manual design at this point.</p>
</blockquote>
<p>计算资源的改进使用允许增加每个阶段的宽度以及阶段的数量而不会计算困难。 利用初始架构的另一种方法是创建稍微低劣但计算更便宜的版本。 我们发现，所有包括旋钮和控制杆在内的计算资源都可以控制平衡，可以使网络的速度比采用非Inception架构的类似网络快2-3倍，但这需要仔细的手动设计。</p>
<h2 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5 GoogLeNet"></a>5 GoogLeNet</h2><blockquote>
<p>We chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to Yann LeCuns pioneering LeNet 5 network. We also use GoogLeNet to refer to the particular incarnation of the Inception architecture used in our submission for the competition. We have also used a deeper and wider Inception network, the quality of which was slightly inferior, but adding it to the ensemble seemed to improve the results marginally. We omit the details of that network, since our experiments have shown that the influence of the exact architectural parameters is relatively minor. Here, the most successful particular instance (named GoogLeNet) is described in Table 1 for demonstrational purposes. The exact same topology (trained with different sampling methods) was used for 6 out of the 7 models in our ensemble.</p>
</blockquote>
<p>我们选择了GoogLeNet作为我们在ILSVRC14比赛中的团队名称。 这个名字是对Yann LeCuns开创LeNet 5网络的致敬。 我们还使用GoogLeNet来指代我们在竞赛中使用的初始架构的特定化身。 我们还使用了更深入，更广泛的Inception网络，其质量稍差，但将其添加到合奏中似乎稍微改善了结果。 我们省略了该网络的细节，因为我们的实验已经表明，确切的架构参数的影响相对较小。 这里，为了示范目的，在表1中描述了最成功的特定实例（名为GoogLeNet）。 在我们的集合中，7个模型中的6个使用了完全相同的拓扑（用不同的采样方法训练）。</p>
<p><img src="Going%20deeper%20with%20convolutions.assets/1529823104031.png" alt="1529823104031"></p>
<blockquote>
<p>All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 taking RGB color channels with mean sub- traction. “#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 filters in the pro- jection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well.</p>
</blockquote>
<p>所有卷积，包括Inception模块中的卷积，都使用ReLU激活。 我们的网络中接收域的大小为224×224，RGB色彩通道的平均减小。 “＃3×3缩小”和“＃5×5缩小”表示在3×3和5×5卷积之前使用的缩小层中的1×1滤波器的数量。 可以在pool proj列中的内置最大池之后看到投影层中1×1个过滤器的数量。 所有这些还原/投影层也使用整流线性激活。</p>
<blockquote>
<p>The network was designed with computational efficiency and practicality in mind, so that inference can be run on individual devices including even those with limited computational resources, especially with low-memory footprint. The network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100. However this number depends on the machine learning infrastructure system used. The use of average pooling before the classifier is based on [12], although our implementation differs in that we use an extra linear layer. This enables adapting and fine-tuning our networks for other label sets easily, but it is mostly convenience and we do not expect it to have a major effect. It was found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained essential even after removing the fully connected layers.</p>
</blockquote>
<p>该网络设计时考虑到了计算效率和实用性，因此推理可以在单个设备上运行，甚至包括那些计算资源有限的设备，特别是在低内存占用情况下。当只计算具有参数的图层时，网络的深度为22层（如果我们还计算池的话，则计算为27层）。用于构建网络的总层数（独立构建块）大约为100.但是，这个数量取决于所使用的机器学习基础设施系统。在分类器之前使用平均池是基于[12]的，尽管我们的实现不同之处在于我们使用了一个额外的线性层。这使得我们的网络能够很容易地适应和微调我们的网络，但它主要是方便的，我们并不期望它有重大影响。研究发现，从完全连接的层转移到平均汇聚将前1精度提高了约0.6％，但即使在去除完全连接的层之后仍然使用丢失。</p>
<blockquote>
<p>Given the relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern. One interesting insight is that the strong performance of relatively shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative. By adding auxiliary classifiers connected to these intermediate layers, we would expect to encourage discrimination in the lower stages in the classifier, increase the gradient signal that gets propagated back, and provide additional regularization. These classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded.</p>
</blockquote>
<p>鉴于网络的深度相对较大，以有效方式将梯度传播回所有层的能力值得关注。 一个有趣的见解是，相对较浅的网络在这个任务上的强大表现表明，网络中间的层产生的特征应该是非常有区别性的。 通过添加连接到这些中间层的辅助分类器，我们期望在分类器的较低阶段鼓励区分，增加传播回来的梯度信号，并提供额外的正则化。 这些分类器采用更小的卷积网络形式，放在初始（4a）和（4d）模块的输出之上。 在训练过程中，他们的损失以折扣权重加入网络的全部损失中（辅助分类器的损失加权为0.3）。 在推断时，这些辅助网络被丢弃。</p>
<blockquote>
<p>The exact structure of the extra network on the side, including the auxiliary classifier, is as follows:</p>
<ul>
<li>An average pooling layer with 5×5 filter size and stride 3, resulting in an 4×4×512 output for the (4a), and 4×4×528 for the (4d) stage.</li>
<li>A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.</li>
<li>A fully connected layer with 1024 units and rectified linear activation.</li>
<li>A dropout layer with 70% ratio of dropped outputs.</li>
<li>A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but removed at inference time).</li>
</ul>
<p>A schematic view of the resulting network is depicted in Figure 3.</p>
</blockquote>
<p>包括辅助分类器在内的附加网络的确切结构如下：</p>
<ul>
<li>具有5×5滤波器大小和步幅3的平均汇聚层，导致（4a）的4×4×512输出和（4d）阶段的4×4×528输出。</li>
<li>具有128个滤波器的1×1卷积，用于降低维度和校正线性激活。</li>
<li>具有1024个单元的完全连接层和整流线性激活。</li>
<li>丢失输出的比例为70％的丢失层。</li>
<li>作为分类器的具有softmax损失的线性层（预测与主分类器相同的1000个类，但在推断时被去除）。</li>
</ul>
<p>图3描绘了最终网络的示意图。</p>
<p><img src="Going%20deeper%20with%20convolutions.assets/6.png" alt="img"> </p>
<p><strong>清晰图请看原论文</strong></p>
<h2 id="6-训练方法-Training-Methodology"><a href="#6-训练方法-Training-Methodology" class="headerlink" title="6 训练方法 Training Methodology"></a>6 训练方法 Training Methodology</h2><blockquote>
<p>Our networks were trained using the DistBelief  distributed machine learning system using modest amount of model and data-parallelism. Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage. Our training used asynchronous stochastic gradient descent with 0.9 momentum , fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging was used to create the final model used at inference time.</p>
</blockquote>
<p>我们的网络使用DistBelief分布式机器学习系统，使用适量的模型和数据并行性进行训练。 虽然我们只使用基于CPU的实现方式，但粗略估计表明，GoogLeNet网络可以在一周内使用少量高端GPU进行融合培训，主要限制是内存使用量。 我们的训练使用0.9动量的异步随机梯度下降，固定的学习率计划（每8个学习时间将学习率降低4％）。 使用Polyak求平均值来创建用于推断时间的最终模型。</p>
<blockquote>
<p>Our image sampling methods have changed substantially over the months leading to the competition, and already converged models were trained on with other options, sometimes in conjunction with changed hyperparameters, like dropout and learning rate, so it is hard to give a definitive guidance to the most effective single way to train these networks. To complicate matters further, some of the models were mainly trained on smaller relative crops, others on larger ones, inspired by [8]. Still, one prescription that was verified to work very well after the competition includes sampling of various sized patches of the image whose size is distributed evenly between 8% and 100% of the image area and whose aspect ratio is chosen randomly between 3/4 and 4/3. Also, we found that the photometric distortions by Andrew Howard [8] were useful to combat overfitting to some extent. In addition, we started to use random interpolation methods (bilinear, area, nearest neighbor and cubic, with equal probability) for resizing relatively late and in conjunction with other hyperparameter changes, so we could not tell definitely whether the final results were affected positively by their use.</p>
</blockquote>
<p>我们的图像采样方法在引入竞争的几个月内已经发生了很大变化，并且已经收敛的模型已经通过其他选项进行了培训，有时还会与更改的超参数一起使用，例如辍学率和学习率等，因此很难给出明确的指导最有效的单一方式来训练这些网络。使问题更加复杂化的是，一些模型主要是针对较小的相关作物进行培训，另一些则针对较大的模型进行培训，其灵感来自[8]。尽管如此，经过验证在竞赛结束后仍能很好地工作的一个处方包括对尺寸均匀分布在图像区域的8％和100％之间的各种尺寸的图像块进行采样，并且其纵横比在3/4和4/3。此外，我们发现安德鲁霍华德[8]的光度扭曲在某种程度上有助于与过度拟合作斗争。此外，我们开始使用随机插值方法（双线性，面积，最近邻和立方，等概率）来调整相对较晚的时间并结合其他超参数变化，因此我们无法确定最终结果是否受到正面影响他们的使用。</p>
<h2 id="7-ILSVRC-2014分类挑战设置和结果-ILSVRC-2014-Classification-Challenge-Setup-and-Results"><a href="#7-ILSVRC-2014分类挑战设置和结果-ILSVRC-2014-Classification-Challenge-Setup-and-Results" class="headerlink" title="7 ILSVRC 2014分类挑战设置和结果( ILSVRC 2014 Classification Challenge Setup and Results)"></a>7 ILSVRC 2014分类挑战设置和结果( ILSVRC 2014 Classification Challenge Setup and Results)</h2><blockquote>
<p>The ILSVRC 2014 classification challenge involves the task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing. Each image is associated with one ground truth category, and performance is measured based on the highest scoring classifier predictions. Two numbers are usually reported: the top-1 accuracy rate, which compares the ground truth against the first predicted class, and the top-5 error rate, which compares the ground truth against the first 5 predicted classes: an image is deemed correctly classified if the ground truth is among the top-5, regardless of its rank in them. The challenge uses the top-5 error rate for ranking purposes.</p>
</blockquote>
<p>ILSVRC 2014分类挑战涉及将图像分类到Imagenet层次结构中1000个叶节点类别之一的任务。 大约有120万张培训图片，50,000张图片用于验证，100,000张图片用于测试。 每个图像都与一个地面真值类别相关联，并且根据最高得分分类器预测来衡量性能。 通常会报告两个数字：前1个准确率，将实际情况与第一个预测类别进行比较，前5个错误率，将实际情况与前5个预测类别进行比较：图像被视为正确分类 如果基础真实在前五名之内，不管它们的排名如何。 挑战使用排名前5的错误率。</p>
<blockquote>
<p>We participated in the challenge with no external data used for training. In addition to the training techniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a higher performance, which we elaborate below.</p>
</blockquote>
<p>我们参加了挑战赛，没有用于训练的外部数据。 除了本文提到的训练技术之外，我们在测试过程中采用了一套技术来获得更高的性能，我们将在下面进行详细介绍。</p>
<blockquote>
<ol>
<li>We independently trained 7 versions of the same GoogLeNet model (including one wider version), and performed ensemble prediction with them. These models were trained with the same initialization (even with the same initial weights, mainly because of an oversight) and learning rate policies, and they only differ in sampling methodologies and the random order in which they see input images.</li>
</ol>
</blockquote>
<ol>
<li>我们独立训练了同一个GoogLeNet模型的7个版本（包括一个更宽的版本），并对它们进行了集合预测。 这些模型经过相同的初始化（即使具有相同的初始权重，主要是因为疏忽）和学习率策略进行了培训，并且它们仅在采样方法和输入图像的随机顺序上有所不同。</li>
</ol>
<blockquote>
<ol>
<li>During testing, we adopted a more aggressive cropping approach than that of Krizhevsky. Specifically, we resize the image to 4 scales where the shorter dimension (height or width) is 256, 288, 320 and 352 respectively, take the left, center and right square of these resized images (in the case of portrait images, we take the top, center and bottom squares). For each square, we then take the 4 corners and the center 224×224 crop as well as the square resized to 224×224, and their mirrored versions. This results in 4×3×6×2 = 144 crops per image. A similar approach was used by Andrew Howard in the previous year’s entry, which we empirically verified to perform slightly worse than the proposed scheme. We note that such aggressive cropping may not be necessary in real applications, as the benefit of more crops becomes marginal after a reasonable number of crops are present (as we will show later on).</li>
</ol>
</blockquote>
<ol>
<li>在测试过程中，我们采取了比Krizhevsky更加激进的裁剪方法。 具体而言，我们将图像调整为4个比例尺，其中较短的尺寸（高度或宽度）分别为256,288,320和352，将这些调整大小的图像的左侧，中间和右侧平方（在人像图像的情况下， 顶部，中心和底部正方形）。 对于每个广场，我们然后采取4个角落和中心224×224作物以及方形调整到224×224和他们的镜像版本。 这导致每张图像4×3×6×2 = 144作物。 安德鲁霍华德在前一年的进入中使用了类似的方法，我们根据经验证实其执行情况比拟议方案略差。 我们注意到，在实际应用中，这种侵略性耕作可能不是必要的，因为在合理数量的作物出现后，更多作物的收益会变得微不足道（正如我们稍后会展示的）。</li>
</ol>
<p><img src="Going%20deeper%20with%20convolutions.assets/1529823970295.png" alt="1529823970295"></p>
<p><img src="Going%20deeper%20with%20convolutions.assets/1529823993150.png" alt="1529823993150"></p>
<blockquote>
<ol>
<li>The softmax probabilities are averaged over multiple crops and over all the individual clas- sifiers to obtain the final prediction. In our experiments we analyzed alternative approaches on the validation data, such as max pooling over crops and averaging over classifiers, but they lead to inferior performance than the simple averaging.</li>
</ol>
</blockquote>
<ol>
<li>softmax概率在多个作物和所有单个分类器上取平均值，以获得最终预测结果。 在我们的实验中，我们分析了验证数据的其他方法，例如作物上的最大汇集和分类器平均，但是它们导致的性能低于简单的平均值。</li>
</ol>
<blockquote>
<p>In the remainder of this paper, we analyze the multiple factors that contribute to the overall performance of the final submission.</p>
</blockquote>
<p>在本文的其余部分，我们分析了导致最终提交的整体表现的多个因素。</p>
<blockquote>
<p>Our final submission in the challenge obtains a top-5 error of 6.67% on both the validation and testing data, ranking the first among other participants. This is a 56.5% relative reduction compared to the SuperVision approach in 2012, and about 40% relative reduction compared to the previous year’s best approach (Clarifai), both of which used external data for training the classifiers. The following table shows the statistics of some of the top-performing approaches.</p>
</blockquote>
<p>我们在挑战中提交的最终提交数据在验证和测试数据上获得了6.67％的前5位错误，在其他参与者中排名第一。 与2012年SuperVision方法相比，这相比减少了56.5％，与前一年的最佳方法（Clarifai）相比，相对减少了约40％，两者均使用外部数据来训练分类器。 下表显示了一些高性能方法的统计数据。</p>
<blockquote>
<p>We also analyze and report the performance of multiple testing choices, by varying the number of models and the number of crops used when predicting an image in the following table. When we use one model, we chose the one with the lowest top-1 error rate on the validation data. All numbers are reported on the validation dataset in order to not overfit to the testing data statistics.</p>
</blockquote>
<p>我们还通过在下表中预测图像时通过改变模型的数量和使用的作物数量来分析和报告多种测试选择的性能。 当我们使用一个模型时，我们选择了验证数据中具有最低前1个错误率的模型。 所有数据都在验证数据集上报告，以避免过度使用测试数据统计。</p>
<h2 id="8-ILSVRC-2014检测挑战设置和结果-ILSVRC-2014-Detection-Challenge-Setup-and-Results"><a href="#8-ILSVRC-2014检测挑战设置和结果-ILSVRC-2014-Detection-Challenge-Setup-and-Results" class="headerlink" title="8 ILSVRC 2014检测挑战设置和结果(ILSVRC 2014 Detection Challenge Setup and Results)"></a>8 ILSVRC 2014检测挑战设置和结果(ILSVRC 2014 Detection Challenge Setup and Results)</h2><blockquote>
<p>The ILSVRC detection task is to produce bounding boxes around objects in images among 200 possible classes. Detected objects count as correct if they match the class of the groundtruth and their bounding boxes overlap by at least 50% (using the Jaccard index). Extraneous detections count as false positives and are penalized. Contrary to the classification task, each image may contain many objects or none, and their scale may vary from large to tiny. Results are reported using the mean average precision (mAP).</p>
</blockquote>
<p>ILSVRC检测任务是在200个可能的类中为图像中的物体生成边界框。 如果检测到的对象与groundtruth的类相匹配，并且它们的边界框重叠至少50％（使用Jaccard索引），则检测到的对象计数为正确。 无关的检测被视为误报，并受到惩罚。 与分类任务相反，每个图像可能包含很多对象或不包含任何对象，并且它们的比例可能会从大到小变化。 结果使用平均平均精确度（mAP）报告。</p>
<p><img src="Going%20deeper%20with%20convolutions/1529824251348.png" alt="1529824251348"></p>
<p><img src="Going%20deeper%20with%20convolutions/1529824266331.png" alt="1529824266331"></p>
<blockquote>
<p>The approach taken by GoogLeNet for detection is similar to the R-CNN by [6], but is augmented with the Inception model as the region classifier. Additionally, the region proposal step is improved by combining the Selective Search [20] approach with multi-box predictions for higher object bounding box recall. In order to cut down the number of false positives, the superpixel size was increased by 2×. This halves the proposals coming from the selective search algorithm. We added back 200 region proposals coming from multi-box [5] resulting, in total, in about 60% of the pro- posals used by [6], while increasing the coverage from 92% to 93%. The overall effect of cutting the number of proposals with increased coverage is a 1% improvement of the mean average precision for the single model case. Finally, we use an ensemble of 6 ConvNets when classifying each region which improves results from 40% to 43.9% accuracy. Note that contrary to R-CNN, we did not use bounding box regression due to lack of time.</p>
</blockquote>
<p>GoogLeNet采用的方法与[6]中的R-CNN类似，但是将Inception模型作为区域分类器进行了扩展。此外，区域提议步骤通过将选择性搜索[20]方法与多框预测相结合来提高对象边界框的回忆。为了减少误报数量，超像素尺寸增加了2倍。这减少了来自选择性搜索算法的提议。我们添加了来自多框[5]的200个区域提案，总共导致了[6]使用的大约60％的提案，同时将覆盖率从92％提高到了93％。削减覆盖率提高的提案数量的总体影响是单个模型案例的平均精确度提高1％。最后，在对每个区域进行分类时，我们使用6个ConvNets集合，从而将结果从40％提高到43.9％的准确度。请注意，与R-CNN相反，由于时间不够，我们没有使用边界框回归。</p>
<blockquote>
<p>We first report the top detection results and show the progress since the first edition of the detection task. Compared to the 2013 result, the accuracy has almost doubled. The top performing teams all use Convolutional Networks. We report the official scores in Table 4 and common strategies for each team: the use of external data, ensemble models or contextual models. The external data is typically the ILSVRC12 classification data for pre-training a model that is later refined on the detection data. Some teams also mention the use of the localization data. Since a good portion of the localization task bounding boxes are not included in the detection dataset, one can pre-train a general bounding box regressor with this data the same way classification is used for pre-training. The GoogLeNet entry did not use the localization data for pretraining.</p>
</blockquote>
<p>我们首先报告最高检测结果并显示检测任务第一版以来的进展情况。 与2013年的结果相比，准确率几乎翻了一番。 表现最佳的团队都使用卷积网络。 我们报告表4中的官方分数以及每个团队的常用策略：使用外部数据，整体模型或上下文模型。 外部数据通常是ILSVRC12分类数据，用于预先培训一个模型，该模型稍后将根据检测数据进行细化。 一些团队还提到了本地化数据的使用。 由于本地化任务边界框的很大一部分不包含在检测数据集中，因此可以用这种数据预训练一般边界框回归器，这与使用分类用于预训练的方式相同。 GoogLeNet条目未将本地化数据用于预训练。</p>
<blockquote>
<p>In Table 5, we compare results using a single model only. The top performing model is by Deep Insight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the GoogLeNet obtains significantly stronger results with the ensemble.</p>
</blockquote>
<p>在表5中，我们仅使用单一模型比较结果。 表现最佳的模型由Deep Insight提供，令人惊讶的是只有3个模型的合奏提高了0.3分，而GoogLeNet获得了更强的合奏效果。</p>
<h2 id="9-结论-Conclusions"><a href="#9-结论-Conclusions" class="headerlink" title="9 结论 Conclusions"></a>9 结论 Conclusions</h2><blockquote>
<p>Our results seem to yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision. The main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallower and less wide networks. Also note that our detection work was competitive despite of neither utilizing context nor performing bounding box regression and this fact provides further evidence of the strength of the Inception architecture. Although it is expected that similar quality of result can be achieved by much more expensive networks of similar depth and width, our approach yields solid evidence that moving to sparser architectures is feasible and useful idea in general. This suggest promising future work towards creating sparser and more refined structures in automated ways on the basis of [2].</p>
</blockquote>
<p>我们的研究结果似乎产生了一个可靠的证据，即通过容易获得的密集构建块来近似预期的最优稀疏结构是改进用于计算机视觉的神经网络的可行方法。 这种方法的主要优点是与较浅和较宽的网络相比，计算要求的适度增加显着提高了质量。 还要注意，尽管我们的检测工作既没有利用上下文，也没有执行边界框回归，但我们的检测工作仍然具有竞争力，这一事实进一步证明了Inception架构的优势。 尽管预计类似的结果质量可以通过更加昂贵的类似深度和宽度的网络来实现，但我们的方法提供了可靠的证据，表明向更稀疏的架构转变是一种可行和有用的想法。 这表明在[2]的基础上有希望的未来工作是以自动化的方式创建稀疏和更精细的结构。</p>
<h2 id="10-致谢-Acknowledgements"><a href="#10-致谢-Acknowledgements" class="headerlink" title="10 致谢 Acknowledgements"></a>10 致谢 Acknowledgements</h2><blockquote>
<p>We would like to thank Sanjeev Arora and Aditya Bhaskara for fruitful discussions on [2]. Also we are indebted to the DistBelief team for their support especially to Rajat Monga, Jon Shlens, Alex Krizhevsky, Jeff Dean, Ilya Sutskever and Andrea Frome. We would also like to thank to Tom Duerig and Ning Ye for their help on photometric distortions. Also our work would not have been possible without the support of Chuck Rosenberg and Hartwig Adam.</p>
</blockquote>
<p>我们要感谢Sanjeev Arora和Aditya Bhaskara [2]的富有成效的讨论。 我们也感谢DistBelief团队对Rajat Monga，Jon Shlens，Alex Krizhevsky，Jeff Dean，Ilya Sutskever和Andrea Frome的支持。 我们还要感谢汤姆杜里格和宁冶对光度失真的帮助。 如果没有Chuck Rosenberg和Hartwig Adam的支持，我们的工作也不可能实现。</p>
<p>原文：<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">https://arxiv.org/abs/1409.4842</a></p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/12/Going deeper with convolutions/" rel="next" title="">
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/12/tt1/" rel="prev" title="">
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Going-deeper-with-convolutions"><span class="nav-number">1.</span> <span class="nav-text">Going deeper with convolutions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要-Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">摘要 Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-介绍-Introduction"><span class="nav-number">1.2.</span> <span class="nav-text">1 介绍 Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-相关工作-Related-Work"><span class="nav-number">1.3.</span> <span class="nav-text">2 相关工作 Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-动机和高层次考虑-Motivation-and-High-Level-Considerations"><span class="nav-number">1.4.</span> <span class="nav-text">3 动机和高层次考虑 Motivation and High Level Considerations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-架构细节-Architectural-Details"><span class="nav-number">1.5.</span> <span class="nav-text">4 架构细节 Architectural Details</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-GoogLeNet"><span class="nav-number">1.6.</span> <span class="nav-text">5 GoogLeNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-训练方法-Training-Methodology"><span class="nav-number">1.7.</span> <span class="nav-text">6 训练方法 Training Methodology</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-ILSVRC-2014分类挑战设置和结果-ILSVRC-2014-Classification-Challenge-Setup-and-Results"><span class="nav-number">1.8.</span> <span class="nav-text">7 ILSVRC 2014分类挑战设置和结果( ILSVRC 2014 Classification Challenge Setup and Results)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-ILSVRC-2014检测挑战设置和结果-ILSVRC-2014-Detection-Challenge-Setup-and-Results"><span class="nav-number">1.9.</span> <span class="nav-text">8 ILSVRC 2014检测挑战设置和结果(ILSVRC 2014 Detection Challenge Setup and Results)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-结论-Conclusions"><span class="nav-number">1.10.</span> <span class="nav-text">9 结论 Conclusions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-致谢-Acknowledgements"><span class="nav-number">1.11.</span> <span class="nav-text">10 致谢 Acknowledgements</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Pisces</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  





  










  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
